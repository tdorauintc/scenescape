#!/bin/bash

# Copyright (C) 2023-2024 Intel Corporation
#
# This software and the related documents are Intel copyrighted materials,
# and your use of them is governed by the express license under which they
# were provided to you ("License"). Unless the License provides otherwise,
# you may not use, modify, copy, publish, distribute, disclose or transmit
# this software or the related documents without Intel's prior written permission.
#
# This software and the related documents are provided as is, with no express
# or implied warranties, other than those that are expressly stated in the License.

TEST_NAME="SAIL-T611"

check_ubuntu_and_kernel_version() {
    # Get the distribution information
    distro_name=$(lsb_release -si)
    distro_version=$(lsb_release -sr)
    # Check if the distribution is Ubuntu 22.04 or 24.04
    if [ "$distro_name" == "Ubuntu" ] && ( [ "$distro_version" == "22.04" ] || [ "$distro_version" == "24.04" ] ); then
        # Get the kernel version
        kernel_version=$(uname -r)
        # Define the required kernel version
        required_kernel_version="6.8"
        # Check if the kernel version is greater than or equal to the required version
        if [ "$(printf '%s\n' "$kernel_version" "$required_kernel_version" | sort -V | head -n1)" == "$required_kernel_version" ]; then
            echo "System is running Ubuntu $distro_version with kernel version $kernel_version (greater than or equal to $required_kernel_version)"
            ENV_GPU_COMPUTE="-e NEOReadDebugKeys=1 -e OverrideGpuAddressSpace=48"
            COUNTER=0
        else
            echo "System is running Ubuntu $distro_version with kernel version $kernel_version (less than $required_kernel_version)"
            ENV_GPU_COMPUTE=""
            COUNTER=-1
        fi
    else
        echo "System is not running Ubuntu 22.04 or 24.04"
        ENV_GPU_COMPUTE=""
        COUNTER=-1
    fi
}

check_ubuntu_and_kernel_version

function start_gpu_measurement()
{
  GPU_ID=$1
  docker run --privileged ${ENV_GPU_COMPUTE} -v ${PWD}:/workspace --tty \
    scenescape:latest --super-shell \
    "tests/percebro_tests/test_measure_gpu ${GPU_ID}" > gpu_data_${GPU_ID}.txt
}

function run_test() {
  ARGS=$1

  # Re-use the test from SAIL-T560 since it protects against the test hanging.
  docker run --privileged -v ${PWD}:/workspace --tty \
       -v ${PWD}/models:${MODELDIR} \
       -e MODEL=${MODEL} -e EXTRA_ARGS="${ARGS}" -e FRAMES=300 \
       ${ENV_GPU_COMPUTE} \
       scenescape:latest \
       tests/percebro_tests/tc_run_source "${SOURCE_FILE}"
  RESULT=$?
  if [[ $RESULT -ne 0 ]]; then
    echo "Test for ${ARGS} failed!"
    echo "${TEST_NAME}: FAIL"
    exit 1
  fi
  return $RESULT
}

function wait_for_pids()
{
  WAIT_ON_PIDS=$1

  for pid in ${WAIT_ON_PIDS}
  do
    DONE=0
    while [[ $DONE -eq 0 && ${pid} -ne 0 ]]
    do
      if ps -p ${pid} > /dev/null
      then
        sleep 1
      else
        DONE=1
      fi
    done
  done
}

function compare_float()
{
  EXPECTED=$1
  VALUE=$2
  # Consider measurements < 0.1 as no activity.
  THRESHOLD=0.1

  if [[ ${EXPECTED} -gt 0 ]]
  then
    RESULT=$( awk -v target="${THRESHOLD}" -v result="${VALUE}" 'BEGIN {printf (result>=target?0:1)}' )
  else
    RESULT=$( awk -v target="${THRESHOLD}" -v result="${VALUE}" 'BEGIN {printf (result<=target?0:1)}' )
  fi

  echo $RESULT
}

function verify_gpu_use()
{
  GPU_ID=$1
  VIDEO_EXPECTED=$2
  INFERENCE_EXPECTED=$3

  VIDEO_USE=$( tail -n 1 gpu_data_${GPU_ID}.txt | awk '{print $1}')
  INFERENCE_USE=$( tail -n 1 gpu_data_${GPU_ID}.txt | awk '{print $2}')

  VIDEO_RESULT=$( compare_float ${VIDEO_EXPECTED} ${VIDEO_USE} )
  INFERENCE_RESULT=$( compare_float ${INFERENCE_EXPECTED} ${INFERENCE_USE} )

  RESULT=$(( $VIDEO_RESULT + $INFERENCE_RESULT ))

  if [[ $RESULT -ne 0 ]]
  then
    echo "GPU ${GPU_ID} use mismatch!"
    echo "VIDEO Expected ${VIDEO_EXPECTED} observed ${VIDEO_USE}"
    echo "INFERENCE Expected ${INFERENCE_EXPECTED} observed ${INFERENCE_USE}"
    echo "${TEST_NAME}: FAIL"
    exit 1
  fi
  rm gpu_data_${GPU_ID}.txt

  if [[ ${VIDEO_EXPECTED} -gt 0 ]]
  then
    VIDEO_EXPECTED="non-zero"
  fi
  if [[ ${INFERENCE_EXPECTED} -gt 0 ]]
  then
    INFERENCE_EXPECTED="non-zero"
  fi
  echo "GPU${GPU_ID}: VIDEO Expected ${VIDEO_EXPECTED} observed ${VIDEO_USE} : INFERENCE Expected ${INFERENCE_EXPECTED} observed ${INFERENCE_USE}"
}

function run_test_and_check()
{
  TEST_CMD=$1
  GPU0_MEASURE=$2
  GPU0_VIDEO_EXPECT=$3
  GPU0_INF_EXPECT=$4
  GPU1_MEASURE=$5
  GPU1_VIDEO_EXPECT=$6
  GPU1_INF_EXPECT=$7

  GPU_PID0=0
  GPU_PID1=0
  if [[ $GPU0_MEASURE -ne 0 ]]
  then
    start_gpu_measurement $((COUNTER+1)) &
    GPU_PID0=$!
  fi
  if [[ $GPU1_MEASURE -ne 0 ]]
  then
    start_gpu_measurement $((COUNTER+2)) &
    GPU_PID1=$!
  fi

  # Add the argument required by percebro
  if [[ -n ${TEST_CMD} ]]
  then
    TEST_CMD="--cv_subsystem ${TEST_CMD}"
  fi
  run_test "$TEST_CMD"

  wait_for_pids "${GPU_PID0} ${GPU_PID1}"

  if [[ $GPU0_MEASURE -ne 0 ]]
  then
    verify_gpu_use $((COUNTER+1)) ${GPU0_VIDEO_EXPECT} ${GPU0_INF_EXPECT}
  fi
  if [[ $GPU1_MEASURE -ne 0 ]]
  then
    verify_gpu_use $((COUNTER+2)) ${GPU1_VIDEO_EXPECT} ${GPU1_INF_EXPECT}
  fi
}

GPU_AVAILABLE=$( lspci -v | grep -E "VGA|Display" | grep Intel | wc -l)

SOURCE_FILE=sample_data/qcam1.mp4
MODELDIR=/opt/intel/openvino/deployment_tools/intel_models
GPU_PID=0
# Default model. Will be updated if GPUs are detected
MODEL="retail"

# For readability:
GPU0_ENABLED=1
GPU1_ENABLED=1
GPU1_DISABLED=0
VIDEO_INACTIVE=0
VIDEO_ACTIVE=1
INFER_INACTIVE=0
INFER_ACTIVE=1

echo ""
echo "Executing: ${TEST_NAME}"
echo "GPU Support test: Detected ${GPU_AVAILABLE} GPUs"

# Test decoding on default, check GPU if available, should not be active for decode.
SUBSYSTEM=""
echo ""
echo "Test 1: Running on default subsystem"
run_test_and_check "${SUBSYSTEM}" ${GPU_AVAILABLE} ${VIDEO_INACTIVE} ${INFER_INACTIVE} ${GPU1_DISABLED}


# Test decoding on 'CPU', check GPU if available, should not be active for decode.
SUBSYSTEM="ANY"
echo ""
echo "Test 2: Running on subsystem ${SUBSYSTEM}"
run_test_and_check ${SUBSYSTEM} ${GPU_AVAILABLE} ${VIDEO_ACTIVE} ${INFER_INACTIVE} ${GPU1_DISABLED}

# Test decoding on 'CPU', check GPU if available, should not be active for decode.
SUBSYSTEM="CPU"
echo ""
echo "Test 3: Running on subsystem ${SUBSYSTEM}"
run_test_and_check ${SUBSYSTEM} ${GPU_AVAILABLE} ${VIDEO_INACTIVE} ${INFER_INACTIVE} ${GPU1_DISABLED}


if [[ ${GPU_AVAILABLE} -gt 0 ]]
then
  # Test decoding and inferencing on GPU0
  MODEL="retail=GPU"
  SUBSYSTEM="GPU"
  echo ""
  echo "Test 4: Running on subsystem ${SUBSYSTEM}, model ${MODEL}"

  run_test_and_check ${SUBSYSTEM} ${GPU_AVAILABLE} ${VIDEO_ACTIVE} ${INFER_ACTIVE} ${GPU1_DISABLED}

else
  echo "No GPU available to test!"
fi

if [[ ${GPU_AVAILABLE} -gt 1 ]]
then
  sleep 5

  # Test both Decoding and inferencing on GPU1
  MODEL="retail=GPU.1"
  SUBSYSTEM="GPU.1"
  echo ""
  echo "Test 4a: Running on subsystem ${SUBSYSTEM}, model ${MODEL}"
  run_test_and_check ${SUBSYSTEM} ${GPU0_ENABLED} ${VIDEO_INACTIVE} ${INFER_INACTIVE} ${GPU1_ENABLED} ${VIDEO_ACTIVE} ${INFER_ACTIVE}
  sleep 5

  # Test Decoding on GPU1, inferencing on GPU0
  SUBSYSTEM="GPU.1"
  MODEL="retail=GPU.0"
  echo ""
  echo "Test 4b: Running on subsystem ${SUBSYSTEM}, model ${MODEL}"
  run_test_and_check ${SUBSYSTEM} ${GPU0_ENABLED} ${VIDEO_INACTIVE} ${INFER_ACTIVE} ${GPU1_ENABLED} ${VIDEO_ACTIVE} ${INFER_INACTIVE}
  sleep 5

  # Test Decoding on GPU0, inferencing on GPU1
  SUBSYSTEM="GPU.0"
  MODEL="retail=GPU.1"
  echo ""
  echo "Test 4c: Running on subsystem ${SUBSYSTEM}, model ${MODEL}"
  run_test_and_check ${SUBSYSTEM} ${GPU0_ENABLED} ${VIDEO_ACTIVE} ${INFER_INACTIVE} ${GPU1_ENABLED} ${VIDEO_INACTIVE} ${INFER_ACTIVE}
fi

echo "${TEST_NAME}: PASS"
exit 0
